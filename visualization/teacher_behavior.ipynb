{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 观察 teacher 的预测行为"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. 加载数据集"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "import torch\n",
    "import registry\n",
    "import datafree\n",
    "print(registry.__file__)\n",
    "print(datafree.__file__)\n",
    "\n",
    "dataset='imagenet'\n",
    "data_root='datasets/'\n",
    "num_classes, ori_dataset, val_dataset = registry.get_dataset(name='imagenet', data_root=data_root)\n",
    "val_loader = torch.utils.data.DataLoader(val_dataset,batch_size=256, shuffle=True,num_workers=0, pin_memory=False)\n",
    "evaluator = datafree.evaluators.classification_evaluator(val_loader)\n",
    "print('==>loading dataset success')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. 加载teacher"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "teacher = registry.get_model('resnet50_imagenet', num_classes=num_classes, pretrained=True).eval()\n",
    "normalizer = datafree.utils.Normalizer(**registry.NORMALIZE_DICT[dataset])\n",
    "print('==>loading teacher success')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. 评估精度"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# device=0\n",
    "# teacher.cuda()\n",
    "# eval_results = evaluator(teacher, device=0)\n",
    "# (acc1, acc5), val_loss = eval_results['Acc'], eval_results['Loss']\n",
    "# print('[teacher] Acc@1={:.4f} Acc@5={:.4f} Loss={:.4f}'.format(acc1,acc5,val_loss))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5. 选出样本"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "num=20\n",
    "teacher.cuda()\n",
    "for i, (inputs, targets) in enumerate( tqdm(val_loader, disable=True) ):\n",
    "        inputs, targets = inputs.cuda(), targets.cuda()\n",
    "        outputs = teacher( inputs )\n",
    "        pred=torch.nn.functional.softmax(outputs,dim=1)\n",
    "        confidence,pred_label=pred.max(dim=1)\n",
    "        c_index=(pred_label==targets)\n",
    "        e_index=(pred_label!=targets)\n",
    "        c_images,c_labels,c_confidence=inputs[c_index],targets[c_index],confidence[c_index]\n",
    "        c_images,c_labels,c_confidence=c_images[0:num],c_labels[0:num],c_confidence[0:num]\n",
    "        e_images,ec_labels,e_labels,e_confidence=inputs[e_index],targets[e_index],pred_label[e_index],confidence[e_index]\n",
    "        e_images,ec_labels,e_labels,e_confidence=e_images[0:num],ec_labels[0:num],e_labels[0:num],e_confidence[0:num]\n",
    "        break\n",
    "for i in range(num):\n",
    "        print ('Corrent sample {}, Label={}, Confidence={}'.format(i,c_labels[i].item(),c_confidence[i].item()))\n",
    "for i in range(num):\n",
    "        print ('Error sample {}, Label={}, Confidence={}'.format(i,e_labels[i].item(),e_confidence[i].item()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6. 简单展示"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import numpy as np\n",
    "# 转换成RGB图\n",
    "def get_image_batch(imgs,col=None, size=None, pack=True):\n",
    "    if isinstance(imgs, torch.Tensor):\n",
    "        imgs = (imgs.detach().clamp(0, 1).cpu().numpy()*255).astype('uint8')\n",
    "    if pack:\n",
    "        imgs = pack_images( imgs, col=col ).transpose( 1, 2, 0 ).squeeze()\n",
    "        imgs = Image.fromarray( imgs )\n",
    "        if size is not None:\n",
    "            if isinstance(size, (list,tuple)):\n",
    "                imgs = imgs.resize(size)\n",
    "            else:\n",
    "                w, h = imgs.size\n",
    "                max_side = max( h, w )\n",
    "                scale = float(size) / float(max_side)\n",
    "                _w, _h = int(w*scale), int(h*scale)\n",
    "                imgs = imgs.resize([_w, _h])\n",
    "    return imgs\n",
    "\n",
    "def pack_images(images, col=None, channel_last=False, padding=1):\n",
    "    # N, C, H, W\n",
    "    if isinstance(images, (list, tuple) ):\n",
    "        images = np.stack(images, 0)\n",
    "    if channel_last:\n",
    "        images = images.transpose(0,3,1,2) # make it channel first\n",
    "    assert len(images.shape)==4\n",
    "    assert isinstance(images, np.ndarray)\n",
    "\n",
    "    N,C,H,W = images.shape\n",
    "    if col is None:\n",
    "        col = int(math.ceil(math.sqrt(N)))\n",
    "    row = int(math.ceil(N / col))\n",
    "    \n",
    "    pack = np.zeros( (C, H*row+padding*(row-1), W*col+padding*(col-1)), dtype=images.dtype )\n",
    "    for idx, img in enumerate(images):\n",
    "        h = (idx // col) * (H+padding)\n",
    "        w = (idx % col) * (W+padding)\n",
    "        pack[:, h:h+H, w:w+W] = img\n",
    "    return pack"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "\n",
    "print('预测正确图像')\n",
    "imgs1=normalizer.__call__(c_images,reverse=True)\n",
    "imgs1=get_image_batch(imgs1,col=8)\n",
    "plt.imshow(imgs1)\n",
    "plt.show(imgs1)\n",
    "\n",
    "print('预测错误图像')\n",
    "imgs2=normalizer.__call__(e_images,reverse=True)\n",
    "imgs2=get_image_batch(imgs2,col=8)\n",
    "plt.imshow(imgs2)\n",
    "plt.show(imgs2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "7. 逐张图像及置信度"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "imgs1=normalizer.__call__(c_images,reverse=True)\n",
    "imgs1=(imgs1.detach().clamp(0, 1).cpu().numpy()*255).astype('uint8')\n",
    "for idx, img in enumerate(imgs1):\n",
    "    img = img.transpose(2, 1,0)\n",
    "    plt.imshow(img)\n",
    "    plt.axis('off')\n",
    "    plt.margins(0, 0)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig('figs/corrent_label_{}_con_{:.4}.png'.format(c_labels[idx].item(),c_confidence[idx].item()),bbox_inches='tight',pad_inches = 0)\n",
    "\n",
    "imgs2=normalizer.__call__(e_images,reverse=True)\n",
    "imgs2=(imgs2.detach().clamp(0, 1).cpu().numpy()*255).astype('uint8')\n",
    "for idx, img in enumerate(imgs2):\n",
    "    img = img.transpose(2, 1,0)\n",
    "    plt.imshow(img)\n",
    "    plt.axis('off')\n",
    "    plt.margins(0, 0)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig('figs/error_l_{}_el_{}_con_{:.4}.png'.format(ec_labels[idx].item(),e_labels[idx].item(),e_confidence[idx].item()),bbox_inches='tight',pad_inches = 0)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "8. 对抗样本攻击"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CW-L2 Attack\n",
    "# Based on the paper, i.e. not exact same version of the code on https://github.com/carlini/nn_robust_attacks\n",
    "# (1) Binary search method for c, (2) Optimization on tanh space, (3) Choosing method best l2 adversaries is NOT IN THIS CO\n",
    "import time\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "def cw_l2_attack(model, images, labels, targeted=False, c=1e-4, kappa=0, max_iter=1000, learning_rate=0.01) :\n",
    "    st=time.time()\n",
    "    device='cuda'\n",
    "    images = images.to(device)     \n",
    "    labels = labels.to(device)\n",
    "    # Define f-function\n",
    "    def f(x) :\n",
    "        outputs = model(x)\n",
    "        one_hot_labels = torch.eye(len(outputs[0]))[labels.cpu()].to(device)\n",
    "        i, _ = torch.max((1-one_hot_labels)*outputs, dim=1)\n",
    "        j = torch.masked_select(outputs, one_hot_labels.byte())\n",
    "        # If targeted, optimize for making the other class most likely \n",
    "        if targeted :\n",
    "            return torch.clamp(i-j, min=-kappa)\n",
    "        # If untargeted, optimize for making the other class most likely \n",
    "        else :\n",
    "            return torch.clamp(j-i, min=-kappa)\n",
    "    w = torch.zeros_like(images, requires_grad=True).to(device)\n",
    "    optimizer = optim.Adam([w], lr=learning_rate)\n",
    "    prev = 1e10\n",
    "    for step in range(max_iter) :\n",
    "        a = 1/2*(nn.Tanh()(w) + 1)\n",
    "        loss1 = nn.MSELoss(reduction='sum')(a, images)\n",
    "        loss2 = torch.sum(c*f(a))\n",
    "        cost = loss1 + loss2\n",
    "        optimizer.zero_grad()\n",
    "        cost.backward()\n",
    "        optimizer.step()\n",
    "        # Early Stop when loss does not converge.\n",
    "        if step % (max_iter//10) == 0 :\n",
    "            if cost > prev :\n",
    "                print('Attack Stopped due to CONVERGENCE....')\n",
    "                return a\n",
    "            prev = cost\n",
    "        print('- Learning Progress : %2.2f %%        ' %((step+1)/max_iter*100), end='\\r')\n",
    "    attack_images = 1/2*(nn.Tanh()(w) + 1)\n",
    "    et=time.time()\n",
    "    print('\\natack time cost: {:.4f}'.format(et-st))\n",
    "    return attack_images"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "9. 执行攻击"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "teacher.eval()\n",
    "imgs=normalizer.__call__(c_images,reverse=True)\n",
    "labels = c_labels.to('cuda')\n",
    "attacked_images = cw_l2_attack(teacher, imgs, c_labels.cuda(), targeted=False, c=0.1)\n",
    "outputs = teacher( attacked_images.cuda() )\n",
    "pred=torch.nn.functional.softmax(outputs,dim=1)\n",
    " \n",
    "imgs3=normalizer.__call__(attacked_images,reverse=True)\n",
    "imgs3=(imgs3.detach().clamp(0, 1).cpu().numpy()*255).astype('uint8')\n",
    "for idx, img in enumerate(imgs3):\n",
    "    pred_label_attack,confidence_attack=pred[idx].argmax().item(),pred[idx].max().item()\n",
    "    print('attack_l_{}_al_{}_con_{:.4}.png'.format(c_labels[idx].item(),\\\n",
    "        pred_label_attack,confidence_attack))\n",
    "    img = img.transpose(2, 1,0)\n",
    "    plt.imshow(img)\n",
    "    plt.axis('off')\n",
    "    plt.margins(0, 0)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig('figs/attack_l_{}_al_{}_con_{:.4}.png'.format(c_labels[idx].item(),\\\n",
    "        pred_label_attack,confidence_attack,bbox_inches='tight',pad_inches = 0))\n",
    "    break"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
